{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib, cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import math\n",
    "import os\n",
    "import itertools\n",
    "import random\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_2_train = \"Task2Dataset/Training/png/\"\n",
    "task_2_test = \"Task2Dataset/TestWithoutRotations/images/\"\n",
    "task_2_test_annotations = \"Task2Dataset/TestWithoutRotations/annotations/\"\n",
    "\n",
    "task_3_test_annotations = \"Task3AdditionalTestDataset/annotations/\"\n",
    "task_3_test = \"Task3AdditionalTestDataset/images/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "#refactored to class structure to simplify parameter tuning and testing of different matching algorithms, outlier rejection etc\n",
    "class SIFT:\n",
    "    #c_thresh=0.09 to get 0.03 used by lowe as used in paper\n",
    "    def __init__(self, n_octaves=3, n_feat=0, c_thresh=0.04, e_thresh=10, lowe_thresh=0.7, ransac_thresh=5.0, sigma=1.6, k=2, min_matches=10, train_folder=\"\", test_folder=\"\", test_annotation_folder=\"\", matcher=cv2.FlannBasedMatcher()):\n",
    "        \"\"\"\n",
    "        Initialise parameters, by default set to OpenCV default values.\n",
    "        Allows easy testing of parameters by simply calling SIFT(n_octaves=5), for example\n",
    "        \"\"\"\n",
    "        self.n_octaves=n_octaves #num octaves used in computing DoG pyramid\n",
    "        self.n_feat=n_feat #num features to retain when getting keypoints/descriptors\n",
    "        self.c_thresh=c_thresh #contrast threshold used in SIFT feature detection\n",
    "        self.e_thresh=e_thresh #edge threshold in SIFT feature detection\n",
    "        self.lowe_thresh=lowe_thresh #threshold for lowe's ratio test\n",
    "        self.ransac_thresh=ransac_thresh #threshold used in RANSAC outlier rejection\n",
    "        self.sigma=sigma #dictates level of gaussian blur in DoG pyramid\n",
    "        self.k=k #number of nearest neighbours obtained in knnmatch\n",
    "        self.min_matches=min_matches #number of matches required to calculate homography - kinda redundant\n",
    "        self.train_folder=train_folder\n",
    "        self.test_folder=test_folder\n",
    "        self.test_annotation_folder=test_annotation_folder\n",
    "        self.test_image=\"\"\n",
    "        self.train_pts = {} # store keypoints of training images\n",
    "        self.train_desc = {} # store descriptors of training images\n",
    "        self.train_feat_count = {} #store number of features found for training image for scoring mechanism\n",
    "        self.matcher = matcher #FlannBasedMatcher or BFMatcher\n",
    "        self.matches = [] #store retained matches\n",
    "        self.homography_matrix = \"\" #store homography matrix\n",
    "        self.inlier_mask = \"\" #store inlier mask\n",
    "        self.test_annotations = {} #store annotations for test images\n",
    "        self.num_train_images=0\n",
    "\n",
    "\n",
    "    def get_sift_train_features(self):\n",
    "        \"\"\"\n",
    "        Gets SIFT keypoints and descriptors for all images in train folder\n",
    "        Stored under dictionaries for easy access using image name\n",
    "        \"\"\"\n",
    "        \n",
    "        sift = cv2.SIFT_create(self.n_feat, self.n_octaves, self.c_thresh, self.e_thresh, self.sigma)\n",
    "\n",
    "        for train_image in os.listdir(self.train_folder):\n",
    "            \n",
    "            image = cv2.imread(os.path.join(self.train_folder, train_image), 0)\n",
    "            pt, desc = sift.detectAndCompute(image, None)\n",
    "            self.train_pts[train_image] = pt\n",
    "            self.train_desc[train_image] = desc\n",
    "            self.train_feat_count[train_image] = len(pt)\n",
    "            self.num_train_images+=1\n",
    "        \n",
    "\n",
    "    def get_annotations(self):\n",
    "        \"\"\"\n",
    "        Get annotations for each test image\n",
    "        \"\"\"\n",
    "        for annotation_file in os.listdir(self.test_annotation_folder):\n",
    "            \n",
    "            image_name = os.path.basename(annotation_file)\n",
    "            annotation_file = open(os.path.join(self.test_annotation_folder, annotation_file), \"r\")\n",
    "            \n",
    "            annotation_lines = annotation_file.readlines()\n",
    "            image_annot = []\n",
    "            \n",
    "            for line in annotation_lines:\n",
    "                image_annot.append(line.split(\",\")[0])\n",
    "            \n",
    "            self.test_annotations[image_name] = image_annot\n",
    "    \n",
    "\n",
    "    def sift_matches_lowes(self, train_image_name, canvas):\n",
    "        \"\"\"\n",
    "        Match SIFT features, using Lowe's Ratio Test as a means of outlier rejection\n",
    "        \"\"\"\n",
    "\n",
    "        #Get test image features\n",
    "        print(f\"Test_image: {self.test_image}\\n\")\n",
    "        print(f\"Train_image: {train_image_name}\\n\")\n",
    "\n",
    "        test_img = cv2.imread(self.test_image, 0)\n",
    "\n",
    "        sift = cv2.SIFT_create(self.n_feat, self.n_octaves, self.c_thresh, self.e_thresh, self.sigma)\n",
    "        test_points, test_descriptor = sift.detectAndCompute(test_img, None)\n",
    "        \n",
    "        #match features\n",
    "        matches = self.matcher.knnMatch(self.train_desc[train_image_name], test_descriptor, self.k)\n",
    "\n",
    "        #lowe's ratio test\n",
    "        for one, two in matches:\n",
    "            if one.distance < self.lowe_thresh * two.distance:\n",
    "                self.matches.append([one])\n",
    "        self.draw_boxes(train_image_name, test_points, test_descriptor)\n",
    "        \n",
    "        \n",
    "    def draw_boxes(self, train_image_name, test_pts, test_desc):\n",
    "        assert self.train_desc, \"Descriptors empty, Run SIFT.get_sift_train_features()\"\n",
    "        assert self.train_pts, \"Points empty, run SIFT.get_sift_train_features()\"\n",
    "        assert self.test_image, \"No test image selected\"\n",
    "        canvas = cv2.imread(\"canvas_test.png\")\n",
    "        img1 = cv2.imread(os.path.join(self.train_folder, train_image_name))\n",
    "        h, w = img1.shape[:2]\n",
    "        \n",
    "        img2 = cv2.imread(self.test_image)\n",
    "        train_pts, train_desc = self.train_pts[train_image_name], self.train_desc[train_image_name]\n",
    "        \n",
    "        src_pts = np.float32([train_pts[m[0].queryIdx].pt for m in self.matches]).reshape(-1,1,2)        \n",
    "        dst_pts = np.float32([test_pts[m[0].trainIdx].pt for m in self.matches]).reshape(-1,1,2)\n",
    "        \n",
    "        print(len(src_pts), len(dst_pts))\n",
    "        if len(src_pts) < 4 or len(dst_pts) < 4:\n",
    "            return\n",
    "        M, mask = cv2.findHomography(src_pts, dst_pts, cv2.RANSAC, self.ransac_thresh)\n",
    "        if M is None:\n",
    "            return\n",
    "        matchesMask = mask.ravel().tolist()\n",
    "\n",
    "        pts = np.float32([ [0,0],[0,h-1],[w-1,h-1],[w-1,0] ]).reshape(-1,1,2)\n",
    "        dst = cv2.perspectiveTransform(pts, M)\n",
    "        if np.any(dst < 0) or np.any(dst > 512):\n",
    "            return\n",
    "        print(f\"Train_image shape ({h},{w})\\n\")\n",
    "        print(\"Test_image shape: ({},{})\\n\".format(*img2.shape[:2]))\n",
    "        print(\"Bounding box Vertices: ({})\".format(np.int32(dst)))\n",
    "        \n",
    "        draw_params = dict(matchColor = (0,255,0), # draw matches in green color\n",
    "               singlePointColor = None,\n",
    "               matchesMask = matchesMask, # draw only inliers\n",
    "               flags = 2)\n",
    "        \n",
    "        # img3 = cv2.drawMatches(img1, train_pts, img2, test_pts, self.matches, None, matchesMask=matchesMask)\n",
    "        canvas = cv2.polylines(canvas, [np.int32(dst)], True, (0,0,255), 1, cv2.LINE_AA)\n",
    "        cv2.imwrite(\"./canvas_test.png\", canvas)\n",
    "        # cv2.imshow(\"result\", img3)\n",
    "        # cv2.waitKey()\n",
    "\n",
    "\n",
    "    def sift_matches_ransac(self, train_image_name):\n",
    "        \"\"\"\n",
    "        Match SIFT features, obtain homography matrix and use RANSAC/inlier mask for outlier rejection\n",
    "        \"\"\"\n",
    "        \n",
    "        #Get test image features\n",
    "        test_img = cv2.imread(self.test_image, 0)\n",
    "        sift = cv2.SIFT_create(self.n_feat, self.n_octaves, self.c_thresh, self.e_thresh, self.sigma)\n",
    "        test_points, test_descriptor = sift.detectAndCompute(test_img, None)\n",
    "        #match features\n",
    "        matches = self.matcher.knnMatch(self.train_desc[train_image_name], test_descriptor, self.k)\n",
    "\n",
    "        if len(matches) > self.min_matches:\n",
    "            #if sufficient matches to compute homography, calculate source/dest points\n",
    "            src_pts = np.float32([self.train_pts[train_image_name][m[0].queryIdx].pt for m in matches]).reshape(-1, 1, 2)\n",
    "            dst_pts = np.float32([test_points[m[0].trainIdx].pt for m in matches]).reshape(-1, 1, 2)\n",
    "            \n",
    "            #get homography matrix + mask\n",
    "            \n",
    "            H, mask = cv2.findHomography(src_pts, dst_pts, cv2.RANSAC, self.ransac_thresh)\n",
    "            self.homography_matrix = H\n",
    "            inlier_mask = mask.ravel().tolist()\n",
    "            #extract inlier matches\n",
    "            self.matches = [matches[i] for i in range(len(matches)) if inlier_mask[i]]\n",
    "        else:\n",
    "            #cannot compute homography\n",
    "            self.matches=[]\n",
    "\n",
    "\n",
    "    def compare_scores_annot(self, scores, test_image):\n",
    "        scores = dict(itertools.islice(scores.items(), len(self.test_annotations[test_image])))\n",
    "\n",
    "        true_pos, false_pos, true_neg, false_neg = 0, 0, 0, 0\n",
    "        print(self.test_annotations[test_image])\n",
    "        print(scores)\n",
    "        print(\"-----------------------------\")\n",
    "        for annotation in self.test_annotations[test_image]:\n",
    "\n",
    "            if annotation in scores:\n",
    "                true_pos+=1\n",
    "            else:\n",
    "                false_pos+=1\n",
    "                false_neg+=1\n",
    "        print(test_image, \":\", true_pos, \"/\", (true_pos+false_pos))\n",
    "        \n",
    "        true_neg = self.num_train_images - false_neg - true_pos - false_pos\n",
    "\n",
    "        return true_pos, false_pos, true_neg, false_neg\n",
    "    \n",
    "    def eval(self, lowes=False, normalise=False):\n",
    "        \"\"\"\n",
    "        Get score for a specific test image\n",
    "        Normalise == False: score is purely a ranking of the number of matches between train images and a test image\n",
    "        Normalise == True: normalise number of matches by number of features extracted from train image\n",
    "        If lowes=false, use ransac. Vice versa.\n",
    "        \"\"\"\n",
    "        tp, fp, tn, fn = 0, 0, 0, 0\n",
    "        for test_image in os.listdir(self.test_folder):\n",
    "            self.test_image = os.path.join(self.test_folder, test_image)\n",
    "            scores = {}\n",
    "            \n",
    "            canvas = cv2.imread(self.test_image)\n",
    "            cv2.imwrite(\"./canvas_test.png\", canvas)\n",
    "            for image, desc in self.train_desc.items():\n",
    "                # if image != \"027-gas-station.png\" and image != \"016-house.png\" and image !=\"011-trash.png\":\n",
    "                #     continue\n",
    "                self.matches = []\n",
    "\n",
    "                if lowes:\n",
    "                    self.sift_matches_lowes(image, canvas)\n",
    "                    # break\n",
    "                else:\n",
    "                    self.sift_matches_ransac(image)\n",
    "                if normalise:\n",
    "                    scores[image.split(\"-\")[1].split(\".\")[0]] = len(self.matches) / self.train_feat_count[image]\n",
    "                else:\n",
    "                    scores[image.split(\"-\")[1].split(\".\")[0]] = len(self.matches)\n",
    "            break\n",
    "\n",
    "            scores = dict(sorted(scores.items(), key=lambda x: x[1], reverse=True))\n",
    "            test_image = test_image.split(\".\")[0]+\".txt\"\n",
    "            true_pos, false_pos, true_neg, false_neg = self.compare_scores_annot(scores, test_image)\n",
    "            tp+=true_pos\n",
    "            tn+=true_neg\n",
    "            fp+=false_pos\n",
    "            fn+=false_neg\n",
    "            \n",
    "        # print(\"Accuracy:\", ((tp+tn)/(tp+tn+fp+fn)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test_image: Task2Dataset/TestWithoutRotations/images/test_image_1.png\n",
      "\n",
      "Train_image: 001-lighthouse.png\n",
      "\n",
      "16 16\n",
      "Train_image shape (512,512)\n",
      "\n",
      "Test_image shape: (512,512)\n",
      "\n",
      "Bounding box Vertices: ([[[473 144]]\n",
      "\n",
      " [[472 140]]\n",
      "\n",
      " [[473 144]]\n",
      "\n",
      " [[474 142]]])\n",
      "Test_image: Task2Dataset/TestWithoutRotations/images/test_image_1.png\n",
      "\n",
      "Train_image: 002-bike.png\n",
      "\n",
      "16 16\n",
      "Test_image: Task2Dataset/TestWithoutRotations/images/test_image_1.png\n",
      "\n",
      "Train_image: 003-bridge-1.png\n",
      "\n",
      "23 23\n",
      "Test_image: Task2Dataset/TestWithoutRotations/images/test_image_1.png\n",
      "\n",
      "Train_image: 004-bridge.png\n",
      "\n",
      "20 20\n",
      "Test_image: Task2Dataset/TestWithoutRotations/images/test_image_1.png\n",
      "\n",
      "Train_image: 005-silo.png\n",
      "\n",
      "16 16\n",
      "Test_image: Task2Dataset/TestWithoutRotations/images/test_image_1.png\n",
      "\n",
      "Train_image: 006-church.png\n",
      "\n",
      "1 1\n",
      "Test_image: Task2Dataset/TestWithoutRotations/images/test_image_1.png\n",
      "\n",
      "Train_image: 007-supermarket.png\n",
      "\n",
      "10 10\n",
      "Train_image shape (512,512)\n",
      "\n",
      "Test_image shape: (512,512)\n",
      "\n",
      "Bounding box Vertices: ([[[457 182]]\n",
      "\n",
      " [[456 180]]\n",
      "\n",
      " [[458 181]]\n",
      "\n",
      " [[456 182]]])\n",
      "Test_image: Task2Dataset/TestWithoutRotations/images/test_image_1.png\n",
      "\n",
      "Train_image: 008-courthouse.png\n",
      "\n",
      "9 9\n",
      "Test_image: Task2Dataset/TestWithoutRotations/images/test_image_1.png\n",
      "\n",
      "Train_image: 009-airport.png\n",
      "\n",
      "6 6\n",
      "Train_image shape (512,512)\n",
      "\n",
      "Test_image shape: (512,512)\n",
      "\n",
      "Bounding box Vertices: ([[[158 472]]\n",
      "\n",
      " [[164 489]]\n",
      "\n",
      " [[187 455]]\n",
      "\n",
      " [[174 470]]])\n",
      "Test_image: Task2Dataset/TestWithoutRotations/images/test_image_1.png\n",
      "\n",
      "Train_image: 010-bench.png\n",
      "\n",
      "15 15\n",
      "Test_image: Task2Dataset/TestWithoutRotations/images/test_image_1.png\n",
      "\n",
      "Train_image: 011-trash.png\n",
      "\n",
      "6 6\n",
      "Test_image: Task2Dataset/TestWithoutRotations/images/test_image_1.png\n",
      "\n",
      "Train_image: 012-bus.png\n",
      "\n",
      "5 5\n",
      "Test_image: Task2Dataset/TestWithoutRotations/images/test_image_1.png\n",
      "\n",
      "Train_image: 013-water-well.png\n",
      "\n",
      "23 23\n",
      "Train_image shape (512,512)\n",
      "\n",
      "Test_image shape: (512,512)\n",
      "\n",
      "Bounding box Vertices: ([[[147 432]]\n",
      "\n",
      " [[141 414]]\n",
      "\n",
      " [[145 429]]\n",
      "\n",
      " [[155 413]]])\n",
      "Test_image: Task2Dataset/TestWithoutRotations/images/test_image_1.png\n",
      "\n",
      "Train_image: 014-flower.png\n",
      "\n",
      "10 10\n",
      "Test_image: Task2Dataset/TestWithoutRotations/images/test_image_1.png\n",
      "\n",
      "Train_image: 015-barn.png\n",
      "\n",
      "9 9\n",
      "Test_image: Task2Dataset/TestWithoutRotations/images/test_image_1.png\n",
      "\n",
      "Train_image: 016-house.png\n",
      "\n",
      "36 36\n",
      "Train_image shape (512,512)\n",
      "\n",
      "Test_image shape: (512,512)\n",
      "\n",
      "Bounding box Vertices: ([[[430 121]]\n",
      "\n",
      " [[429 184]]\n",
      "\n",
      " [[494 183]]\n",
      "\n",
      " [[493 120]]])\n",
      "Test_image: Task2Dataset/TestWithoutRotations/images/test_image_1.png\n",
      "\n",
      "Train_image: 017-cinema.png\n",
      "\n",
      "10 10\n",
      "Test_image: Task2Dataset/TestWithoutRotations/images/test_image_1.png\n",
      "\n",
      "Train_image: 018-bank.png\n",
      "\n",
      "11 11\n",
      "Test_image: Task2Dataset/TestWithoutRotations/images/test_image_1.png\n",
      "\n",
      "Train_image: 019-prison.png\n",
      "\n",
      "30 30\n",
      "Test_image: Task2Dataset/TestWithoutRotations/images/test_image_1.png\n",
      "\n",
      "Train_image: 020-atm.png\n",
      "\n",
      "8 8\n",
      "Test_image: Task2Dataset/TestWithoutRotations/images/test_image_1.png\n",
      "\n",
      "Train_image: 021-solar-panel.png\n",
      "\n",
      "79 79\n",
      "Test_image: Task2Dataset/TestWithoutRotations/images/test_image_1.png\n",
      "\n",
      "Train_image: 022-car.png\n",
      "\n",
      "10 10\n",
      "Test_image: Task2Dataset/TestWithoutRotations/images/test_image_1.png\n",
      "\n",
      "Train_image: 023-traffic-light.png\n",
      "\n",
      "1 1\n",
      "Test_image: Task2Dataset/TestWithoutRotations/images/test_image_1.png\n",
      "\n",
      "Train_image: 024-fountain.png\n",
      "\n",
      "19 19\n",
      "Test_image: Task2Dataset/TestWithoutRotations/images/test_image_1.png\n",
      "\n",
      "Train_image: 025-factory.png\n",
      "\n",
      "23 23\n",
      "Test_image: Task2Dataset/TestWithoutRotations/images/test_image_1.png\n",
      "\n",
      "Train_image: 026-shop.png\n",
      "\n",
      "2 2\n",
      "Test_image: Task2Dataset/TestWithoutRotations/images/test_image_1.png\n",
      "\n",
      "Train_image: 027-gas-station.png\n",
      "\n",
      "70 70\n",
      "Train_image shape (512,512)\n",
      "\n",
      "Test_image shape: (512,512)\n",
      "\n",
      "Bounding box Vertices: ([[[148 417]]\n",
      "\n",
      " [[148 481]]\n",
      "\n",
      " [[211 481]]\n",
      "\n",
      " [[211 417]]])\n",
      "Test_image: Task2Dataset/TestWithoutRotations/images/test_image_1.png\n",
      "\n",
      "Train_image: 028-government.png\n",
      "\n",
      "2 2\n",
      "Test_image: Task2Dataset/TestWithoutRotations/images/test_image_1.png\n",
      "\n",
      "Train_image: 029-theater.png\n",
      "\n",
      "25 25\n",
      "Train_image shape (512,512)\n",
      "\n",
      "Test_image shape: (512,512)\n",
      "\n",
      "Bounding box Vertices: ([[[432 277]]\n",
      "\n",
      " [[441 327]]\n",
      "\n",
      " [[490 326]]\n",
      "\n",
      " [[499 277]]])\n",
      "Test_image: Task2Dataset/TestWithoutRotations/images/test_image_1.png\n",
      "\n",
      "Train_image: 030-telephone-booth.png\n",
      "\n",
      "18 18\n",
      "Test_image: Task2Dataset/TestWithoutRotations/images/test_image_1.png\n",
      "\n",
      "Train_image: 031-field.png\n",
      "\n",
      "2 2\n",
      "Test_image: Task2Dataset/TestWithoutRotations/images/test_image_1.png\n",
      "\n",
      "Train_image: 032-van.png\n",
      "\n",
      "7 7\n",
      "Test_image: Task2Dataset/TestWithoutRotations/images/test_image_1.png\n",
      "\n",
      "Train_image: 033-hydrant.png\n",
      "\n",
      "44 44\n",
      "Train_image shape (512,512)\n",
      "\n",
      "Test_image shape: (512,512)\n",
      "\n",
      "Bounding box Vertices: ([[[472 144]]\n",
      "\n",
      " [[506 152]]\n",
      "\n",
      " [[473 145]]\n",
      "\n",
      " [[479 144]]])\n",
      "Test_image: Task2Dataset/TestWithoutRotations/images/test_image_1.png\n",
      "\n",
      "Train_image: 034-billboard.png\n",
      "\n",
      "10 10\n",
      "Test_image: Task2Dataset/TestWithoutRotations/images/test_image_1.png\n",
      "\n",
      "Train_image: 035-police.png\n",
      "\n",
      "8 8\n",
      "Test_image: Task2Dataset/TestWithoutRotations/images/test_image_1.png\n",
      "\n",
      "Train_image: 036-hotel.png\n",
      "\n",
      "20 20\n",
      "Test_image: Task2Dataset/TestWithoutRotations/images/test_image_1.png\n",
      "\n",
      "Train_image: 037-post-office.png\n",
      "\n",
      "10 10\n",
      "Test_image: Task2Dataset/TestWithoutRotations/images/test_image_1.png\n",
      "\n",
      "Train_image: 038-library.png\n",
      "\n",
      "3 3\n",
      "Test_image: Task2Dataset/TestWithoutRotations/images/test_image_1.png\n",
      "\n",
      "Train_image: 039-university.png\n",
      "\n",
      "6 6\n",
      "Train_image shape (512,512)\n",
      "\n",
      "Test_image shape: (512,512)\n",
      "\n",
      "Bounding box Vertices: ([[[465 301]]\n",
      "\n",
      " [[490 289]]\n",
      "\n",
      " [[374 345]]\n",
      "\n",
      " [[481 293]]])\n",
      "Test_image: Task2Dataset/TestWithoutRotations/images/test_image_1.png\n",
      "\n",
      "Train_image: 040-bus-stop.png\n",
      "\n",
      "18 18\n",
      "Train_image shape (512,512)\n",
      "\n",
      "Test_image shape: (512,512)\n",
      "\n",
      "Bounding box Vertices: ([[[357 122]]\n",
      "\n",
      " [[414 171]]\n",
      "\n",
      " [[359 123]]\n",
      "\n",
      " [[361 125]]])\n",
      "Test_image: Task2Dataset/TestWithoutRotations/images/test_image_1.png\n",
      "\n",
      "Train_image: 041-windmill.png\n",
      "\n",
      "13 13\n",
      "Train_image shape (512,512)\n",
      "\n",
      "Test_image shape: (512,512)\n",
      "\n",
      "Bounding box Vertices: ([[[478 294]]\n",
      "\n",
      " [[477 294]]\n",
      "\n",
      " [[478 295]]\n",
      "\n",
      " [[478 295]]])\n",
      "Test_image: Task2Dataset/TestWithoutRotations/images/test_image_1.png\n",
      "\n",
      "Train_image: 042-tractor.png\n",
      "\n",
      "34 34\n",
      "Train_image shape (512,512)\n",
      "\n",
      "Test_image shape: (512,512)\n",
      "\n",
      "Bounding box Vertices: ([[[357 122]]\n",
      "\n",
      " [[356 121]]\n",
      "\n",
      " [[349 115]]\n",
      "\n",
      " [[357 121]]])\n",
      "Test_image: Task2Dataset/TestWithoutRotations/images/test_image_1.png\n",
      "\n",
      "Train_image: 043-sign.png\n",
      "\n",
      "14 14\n",
      "Test_image: Task2Dataset/TestWithoutRotations/images/test_image_1.png\n",
      "\n",
      "Train_image: 044-ferris-wheel.png\n",
      "\n",
      "10 10\n",
      "Test_image: Task2Dataset/TestWithoutRotations/images/test_image_1.png\n",
      "\n",
      "Train_image: 045-museum.png\n",
      "\n",
      "6 6\n",
      "Test_image: Task2Dataset/TestWithoutRotations/images/test_image_1.png\n",
      "\n",
      "Train_image: 046-fire-station.png\n",
      "\n",
      "13 13\n",
      "Test_image: Task2Dataset/TestWithoutRotations/images/test_image_1.png\n",
      "\n",
      "Train_image: 047-restaurant.png\n",
      "\n",
      "10 10\n",
      "Test_image: Task2Dataset/TestWithoutRotations/images/test_image_1.png\n",
      "\n",
      "Train_image: 048-hospital.png\n",
      "\n",
      "23 23\n",
      "Train_image shape (512,512)\n",
      "\n",
      "Test_image shape: (512,512)\n",
      "\n",
      "Bounding box Vertices: ([[[462 142]]\n",
      "\n",
      " [[472 155]]\n",
      "\n",
      " [[474 157]]\n",
      "\n",
      " [[472 155]]])\n",
      "Test_image: Task2Dataset/TestWithoutRotations/images/test_image_1.png\n",
      "\n",
      "Train_image: 049-school.png\n",
      "\n",
      "2 2\n",
      "Test_image: Task2Dataset/TestWithoutRotations/images/test_image_1.png\n",
      "\n",
      "Train_image: 050-cemetery.png\n",
      "\n",
      "0 0\n"
     ]
    }
   ],
   "source": [
    "sift = SIFT(train_folder=task_2_train, \n",
    "            test_folder=task_2_test, \n",
    "            test_annotation_folder=task_2_test_annotations, \n",
    "            matcher=cv2.BFMatcher_create(),\n",
    "            lowe_thresh=0.8, min_matches=3, n_feat=1000, n_octaves=5, c_thresh=0.1)\n",
    "\n",
    "sift.get_sift_train_features()\n",
    "sift.eval(normalise=True, lowes=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FLANN with RANSAC - shit\n",
    "sift = SIFT(train_folder=task_2_train, \n",
    "            test_folder=task_2_test, \n",
    "            test_annotation_folder=task_2_test_annotations, \n",
    "            matcher=cv2.FlannBasedMatcher_create(),\n",
    "            lowe_thresh=0.3, min_matches=3, n_feat=1000, n_octaves=5, c_thresh=0.1)\n",
    "\n",
    "sift.get_annotations()\n",
    "sift.get_sift_train_features()\n",
    "sift.eval(normalise=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['gas-station', 'trash', 'theater', 'house']\n",
      "{'gas': 0.17647058823529413, 'theater': 0.06060606060606061, 'house': 0.04, 'atm': 0.025}\n",
      "-----------------------------\n",
      "test_image_1.txt : 2 / 4\n",
      "['factory', 'hotel', 'university', 'bank', 'cinema']\n",
      "{'hotel': 0.13253012048192772, 'bank': 0.09090909090909091, 'factory': 0.031914893617021274, 'university': 0.021739130434782608, 'hospital': 0.0196078431372549}\n",
      "-----------------------------\n",
      "test_image_10.txt : 4 / 5\n",
      "['supermarket', 'post-office', 'bridge', 'van']\n",
      "{'van': 0.17543859649122806, 'supermarket': 0.16842105263157894, 'post': 0.09210526315789473, 'bridge': 0.06956521739130435}\n",
      "-----------------------------\n",
      "test_image_11.txt : 3 / 4\n",
      "['flower', 'cemetery', 'traffic-light', 'fountain', 'ferris-wheel']\n",
      "{'cemetery': 0.18181818181818182, 'ferris': 0.12962962962962962, 'traffic': 0.05172413793103448, 'flower': 0.04411764705882353, 'lighthouse': 0.0}\n",
      "-----------------------------\n",
      "test_image_12.txt : 2 / 5\n",
      "['government', 'telephone-booth', 'car', 'shop', 'cemetery']\n",
      "{'cemetery': 0.18181818181818182, 'shop': 0.13559322033898305, 'gas': 0.13445378151260504, 'house': 0.09333333333333334, 'telephone': 0.06962025316455696}\n",
      "-----------------------------\n",
      "test_image_13.txt : 2 / 5\n",
      "['police', 'post-office', 'bank', 'university']\n",
      "{'police': 0.18666666666666668, 'bank': 0.11363636363636363, 'post': 0.09210526315789473, 'cinema': 0.03361344537815126}\n",
      "-----------------------------\n",
      "test_image_14.txt : 2 / 4\n",
      "['atm', 'traffic-light', 'water-well', 'prison']\n",
      "{'atm': 0.175, 'church': 0.15, 'water': 0.112, 'library': 0.10909090909090909}\n",
      "-----------------------------\n",
      "test_image_15.txt : 1 / 4\n",
      "['gas-station', 'fire-station', 'billboard', 'church', 'bridge']\n",
      "{'gas': 0.16806722689075632, 'fire': 0.10416666666666667, 'bridge': 0.0782608695652174, 'church': 0.05, 'water': 0.024}\n",
      "-----------------------------\n",
      "test_image_16.txt : 2 / 5\n",
      "['van', 'courthouse', 'police', 'theater', 'bus']\n",
      "{'police': 0.18666666666666668, 'courthouse': 0.1, 'van': 0.08771929824561403, 'theater': 0.06060606060606061, 'hydrant': 0.05042016806722689}\n",
      "-----------------------------\n",
      "test_image_17.txt : 4 / 5\n",
      "['airport', 'traffic-light', 'house', 'car', 'windmill']\n",
      "{'airport': 0.09375, 'traffic': 0.06896551724137931, 'house': 0.05333333333333334, 'windmill': 0.04230769230769231, 'car': 0.019230769230769232}\n",
      "-----------------------------\n",
      "test_image_18.txt : 4 / 5\n",
      "['theater', 'silo', 'flower', 'bridge']\n",
      "{'theater': 0.09090909090909091, 'silo': 0.08108108108108109, 'bridge': 0.06956521739130435, 'hydrant': 0.058823529411764705}\n",
      "-----------------------------\n",
      "test_image_19.txt : 3 / 4\n",
      "['government', 'cemetery', 'airport', 'billboard', 'fire-station']\n",
      "{'cemetery': 0.18181818181818182, 'airport': 0.09375, 'fire': 0.08333333333333333, 'government': 0.045454545454545456, 'billboard': 0.011235955056179775}\n",
      "-----------------------------\n",
      "test_image_2.txt : 4 / 5\n",
      "['courthouse', 'bike', 'police', 'airport']\n",
      "{'bike': 0.19753086419753085, 'police': 0.18666666666666668, 'courthouse': 0.15, 'airport': 0.09375}\n",
      "-----------------------------\n",
      "test_image_20.txt : 4 / 4\n",
      "['supermarket', 'tractor', 'bridge-1', 'water-well']\n",
      "{'supermarket': 0.17894736842105263, 'water': 0.12, 'library': 0.07272727272727272, 'tractor': 0.06896551724137931}\n",
      "-----------------------------\n",
      "test_image_3.txt : 2 / 4\n",
      "['bike', 'police', 'field', 'ferris-wheel', 'library']\n",
      "{'police': 0.18666666666666668, 'bike': 0.16666666666666666, 'field': 0.16666666666666666, 'ferris': 0.12962962962962962, 'library': 0.07272727272727272}\n",
      "-----------------------------\n",
      "test_image_4.txt : 4 / 5\n",
      "['gas-station', 'atm', 'fire-station', 'ferris-wheel', 'bus-stop']\n",
      "{'gas': 0.17647058823529413, 'atm': 0.175, 'ferris': 0.12345679012345678, 'fire': 0.08333333333333333, 'water': 0.024}\n",
      "-----------------------------\n",
      "test_image_5.txt : 1 / 5\n",
      "['post-office', 'atm', 'telephone-booth', 'bike']\n",
      "{'bike': 0.2037037037037037, 'atm': 0.175, 'post': 0.07894736842105263, 'telephone': 0.06329113924050633}\n",
      "-----------------------------\n",
      "test_image_6.txt : 2 / 4\n",
      "['traffic-light', 'atm', 'university', 'fountain', 'car']\n",
      "{'atm': 0.175, 'university': 0.06521739130434782, 'traffic': 0.05172413793103448, 'car': 0.038461538461538464, 'lighthouse': 0.0}\n",
      "-----------------------------\n",
      "test_image_7.txt : 3 / 5\n",
      "['bridge', 'fountain', 'fire-station', 'university', 'telephone-booth']\n",
      "{'fire': 0.08333333333333333, 'telephone': 0.0759493670886076, 'bridge': 0.06086956521739131, 'university': 0.043478260869565216, 'barn': 0.01}\n",
      "-----------------------------\n",
      "test_image_8.txt : 2 / 5\n",
      "['car', 'police', 'cemetery', 'school']\n",
      "{'police': 0.2, 'cemetery': 0.18181818181818182, 'school': 0.1111111111111111, 'cinema': 0.025210084033613446}\n",
      "-----------------------------\n",
      "test_image_9.txt : 3 / 4\n",
      "Accuracy: 0.926\n"
     ]
    }
   ],
   "source": [
    "#bf with lowes\n",
    "sift = SIFT(train_folder=task_2_train, \n",
    "            test_folder=task_2_test, \n",
    "            test_annotation_folder=task_2_test_annotations, \n",
    "            matcher=cv2.BFMatcher_create(),\n",
    "            lowe_thresh=0.3, min_matches=3, n_feat=1000, n_octaves=5, c_thresh=0.1)\n",
    "\n",
    "sift.get_annotations()\n",
    "sift.get_sift_train_features()\n",
    "sift.eval(normalise=True, lowes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['gas-station', 'trash', 'theater', 'house']\n",
      "{'trash': 0.49019607843137253, 'cemetery': 0.45454545454545453, 'church': 0.45, 'field': 0.4444444444444444}\n",
      "-----------------------------\n",
      "test_image_1.txt : 1 / 4\n",
      "['factory', 'hotel', 'university', 'bank', 'cinema']\n",
      "{'trash': 0.49019607843137253, 'cemetery': 0.45454545454545453, 'church': 0.4, 'field': 0.3888888888888889, 'prison': 0.38666666666666666}\n",
      "-----------------------------\n",
      "test_image_10.txt : 0 / 5\n",
      "['supermarket', 'post-office', 'bridge', 'van']\n",
      "{'trash': 0.6274509803921569, 'cemetery': 0.5454545454545454, 'van': 0.5087719298245614, 'church': 0.45}\n",
      "-----------------------------\n",
      "test_image_11.txt : 1 / 4\n",
      "['flower', 'cemetery', 'traffic-light', 'fountain', 'ferris-wheel']\n",
      "{'cemetery': 0.5454545454545454, 'traffic': 0.4482758620689655, 'billboard': 0.42696629213483145, 'museum': 0.4090909090909091, 'courthouse': 0.4}\n",
      "-----------------------------\n",
      "test_image_12.txt : 1 / 5\n",
      "['government', 'telephone-booth', 'car', 'shop', 'cemetery']\n",
      "{'prison': 0.5733333333333334, 'cemetery': 0.5454545454545454, 'silo': 0.5405405405405406, 'trash': 0.49019607843137253, 'fire': 0.4583333333333333}\n",
      "-----------------------------\n",
      "test_image_13.txt : 1 / 5\n",
      "['police', 'post-office', 'bank', 'university']\n",
      "{'solar': 0.6826347305389222, 'trash': 0.6666666666666666, 'cemetery': 0.5454545454545454, 'police': 0.4666666666666667}\n",
      "-----------------------------\n",
      "test_image_14.txt : 1 / 4\n",
      "['atm', 'traffic-light', 'water-well', 'prison']\n",
      "{'cemetery': 0.6363636363636364, 'trash': 0.6078431372549019, 'church': 0.5, 'field': 0.5}\n",
      "-----------------------------\n",
      "test_image_15.txt : 0 / 4\n",
      "['gas-station', 'fire-station', 'billboard', 'church', 'bridge']\n",
      "{'cemetery': 0.6363636363636364, 'trash': 0.6274509803921569, 'field': 0.5555555555555556, 'church': 0.5, 'museum': 0.45454545454545453}\n",
      "-----------------------------\n",
      "test_image_16.txt : 1 / 5\n",
      "['van', 'courthouse', 'police', 'theater', 'bus']\n",
      "{'trash': 0.5686274509803921, 'cemetery': 0.5454545454545454, 'police': 0.49333333333333335, 'fire': 0.4583333333333333, 'prison': 0.41333333333333333}\n",
      "-----------------------------\n",
      "test_image_17.txt : 1 / 5\n",
      "['airport', 'traffic-light', 'house', 'car', 'windmill']\n",
      "{'trash': 0.6470588235294118, 'cemetery': 0.6363636363636364, 'telephone': 0.4873417721518987, 'traffic': 0.4827586206896552, 'silo': 0.45045045045045046}\n",
      "-----------------------------\n",
      "test_image_18.txt : 0 / 5\n",
      "['theater', 'silo', 'flower', 'bridge']\n",
      "{'field': 0.6111111111111112, 'cemetery': 0.5454545454545454, 'trash': 0.5294117647058824, 'fire': 0.4583333333333333}\n",
      "-----------------------------\n",
      "test_image_19.txt : 0 / 4\n",
      "['government', 'cemetery', 'airport', 'billboard', 'fire-station']\n",
      "{'field': 0.6111111111111112, 'solar': 0.592814371257485, 'hydrant': 0.5882352941176471, 'trash': 0.5490196078431373, 'museum': 0.5227272727272727}\n",
      "-----------------------------\n",
      "test_image_2.txt : 0 / 5\n",
      "['courthouse', 'bike', 'police', 'airport']\n",
      "{'trash': 0.5882352941176471, 'police': 0.5733333333333334, 'hydrant': 0.5630252100840336, 'church': 0.5}\n",
      "-----------------------------\n",
      "test_image_20.txt : 1 / 4\n",
      "['supermarket', 'tractor', 'bridge-1', 'water-well']\n",
      "{'trash': 0.6862745098039216, 'government': 0.5454545454545454, 'cemetery': 0.5454545454545454, 'church': 0.5}\n",
      "-----------------------------\n",
      "test_image_3.txt : 0 / 4\n",
      "['bike', 'police', 'field', 'ferris-wheel', 'library']\n",
      "{'field': 0.6111111111111112, 'trash': 0.5686274509803921, 'church': 0.5, 'police': 0.48, 'cemetery': 0.45454545454545453}\n",
      "-----------------------------\n",
      "test_image_4.txt : 2 / 5\n",
      "['gas-station', 'atm', 'fire-station', 'ferris-wheel', 'bus-stop']\n",
      "{'trash': 0.5686274509803921, 'solar': 0.5119760479041916, 'cemetery': 0.45454545454545453, 'church': 0.4, 'atm': 0.4}\n",
      "-----------------------------\n",
      "test_image_5.txt : 1 / 5\n",
      "['post-office', 'atm', 'telephone-booth', 'bike']\n",
      "{'trash': 0.5882352941176471, 'church': 0.5, 'museum': 0.4772727272727273, 'cemetery': 0.45454545454545453}\n",
      "-----------------------------\n",
      "test_image_6.txt : 0 / 4\n",
      "['traffic-light', 'atm', 'university', 'fountain', 'car']\n",
      "{'trash': 0.5686274509803921, 'cemetery': 0.5454545454545454, 'traffic': 0.4827586206896552, 'church': 0.45, 'atm': 0.45}\n",
      "-----------------------------\n",
      "test_image_7.txt : 1 / 5\n",
      "['bridge', 'fountain', 'fire-station', 'university', 'telephone-booth']\n",
      "{'trash': 0.6666666666666666, 'solar': 0.625748502994012, 'university': 0.45652173913043476, 'cemetery': 0.45454545454545453, 'church': 0.45}\n",
      "-----------------------------\n",
      "test_image_8.txt : 1 / 5\n",
      "['car', 'police', 'cemetery', 'school']\n",
      "{'police': 0.6133333333333333, 'trash': 0.6078431372549019, 'museum': 0.5454545454545454, 'telephone': 0.46835443037974683}\n",
      "-----------------------------\n",
      "test_image_9.txt : 1 / 4\n",
      "Accuracy: 0.846\n"
     ]
    }
   ],
   "source": [
    "#bf with ransac\n",
    "sift = SIFT(train_folder=task_2_train, \n",
    "            test_folder=task_2_test, \n",
    "            test_annotation_folder=task_2_test_annotations, \n",
    "            matcher=cv2.BFMatcher_create(),\n",
    "            lowe_thresh=0.3, min_matches=3, n_feat=1000, n_octaves=5, c_thresh=0.1)\n",
    "\n",
    "sift.get_annotations()\n",
    "sift.get_sift_train_features()\n",
    "sift.eval(normalise=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['gas-station', 'trash', 'theater', 'house']\n",
      "{'gas': 0.17647058823529413, 'theater': 0.06060606060606061, 'house': 0.05333333333333334, 'atm': 0.025}\n",
      "-----------------------------\n",
      "test_image_1.txt : 2 / 4\n",
      "['factory', 'hotel', 'university', 'bank', 'cinema']\n",
      "{'hotel': 0.13253012048192772, 'bank': 0.11363636363636363, 'factory': 0.031914893617021274, 'university': 0.021739130434782608, 'hospital': 0.0196078431372549}\n",
      "-----------------------------\n",
      "test_image_10.txt : 4 / 5\n",
      "['supermarket', 'post-office', 'bridge', 'van']\n",
      "{'supermarket': 0.17894736842105263, 'van': 0.17543859649122806, 'post': 0.09210526315789473, 'bridge': 0.06956521739130435}\n",
      "-----------------------------\n",
      "test_image_11.txt : 3 / 4\n",
      "['flower', 'cemetery', 'traffic-light', 'fountain', 'ferris-wheel']\n",
      "{'cemetery': 0.18181818181818182, 'ferris': 0.12962962962962962, 'traffic': 0.05172413793103448, 'flower': 0.04411764705882353, 'lighthouse': 0.0}\n",
      "-----------------------------\n",
      "test_image_12.txt : 2 / 5\n",
      "['government', 'telephone-booth', 'car', 'shop', 'cemetery']\n",
      "{'cemetery': 0.18181818181818182, 'shop': 0.13559322033898305, 'gas': 0.13445378151260504, 'house': 0.09333333333333334, 'telephone': 0.06962025316455696}\n",
      "-----------------------------\n",
      "test_image_13.txt : 2 / 5\n",
      "['police', 'post-office', 'bank', 'university']\n",
      "{'police': 0.18666666666666668, 'bank': 0.11363636363636363, 'post': 0.09210526315789473, 'cinema': 0.03361344537815126}\n",
      "-----------------------------\n",
      "test_image_14.txt : 2 / 4\n",
      "['atm', 'traffic-light', 'water-well', 'prison']\n",
      "{'atm': 0.175, 'church': 0.15, 'water': 0.144, 'library': 0.10909090909090909}\n",
      "-----------------------------\n",
      "test_image_15.txt : 1 / 4\n",
      "['gas-station', 'fire-station', 'billboard', 'church', 'bridge']\n",
      "{'gas': 0.16806722689075632, 'fire': 0.10416666666666667, 'bridge': 0.0782608695652174, 'church': 0.05, 'water': 0.024}\n",
      "-----------------------------\n",
      "test_image_16.txt : 2 / 5\n",
      "['van', 'courthouse', 'police', 'theater', 'bus']\n",
      "{'police': 0.18666666666666668, 'courthouse': 0.1, 'van': 0.08771929824561403, 'theater': 0.06060606060606061, 'hydrant': 0.05042016806722689}\n",
      "-----------------------------\n",
      "test_image_17.txt : 4 / 5\n",
      "['airport', 'traffic-light', 'house', 'car', 'windmill']\n",
      "{'airport': 0.09375, 'traffic': 0.06896551724137931, 'house': 0.05333333333333334, 'windmill': 0.04230769230769231, 'car': 0.019230769230769232}\n",
      "-----------------------------\n",
      "test_image_18.txt : 4 / 5\n",
      "['theater', 'silo', 'flower', 'bridge']\n",
      "{'theater': 0.09090909090909091, 'silo': 0.08108108108108109, 'bridge': 0.06956521739130435, 'hydrant': 0.058823529411764705}\n",
      "-----------------------------\n",
      "test_image_19.txt : 3 / 4\n",
      "['government', 'cemetery', 'airport', 'billboard', 'fire-station']\n",
      "{'cemetery': 0.18181818181818182, 'airport': 0.09375, 'fire': 0.08333333333333333, 'government': 0.045454545454545456, 'billboard': 0.011235955056179775}\n",
      "-----------------------------\n",
      "test_image_2.txt : 4 / 5\n",
      "['courthouse', 'bike', 'police', 'airport']\n",
      "{'bike': 0.2037037037037037, 'police': 0.18666666666666668, 'courthouse': 0.15, 'airport': 0.09375}\n",
      "-----------------------------\n",
      "test_image_20.txt : 4 / 4\n",
      "['supermarket', 'tractor', 'bridge-1', 'water-well']\n",
      "{'supermarket': 0.17894736842105263, 'water': 0.12, 'library': 0.07272727272727272, 'tractor': 0.06896551724137931}\n",
      "-----------------------------\n",
      "test_image_3.txt : 2 / 4\n",
      "['bike', 'police', 'field', 'ferris-wheel', 'library']\n",
      "{'police': 0.18666666666666668, 'bike': 0.1728395061728395, 'field': 0.16666666666666666, 'ferris': 0.12962962962962962, 'library': 0.07272727272727272}\n",
      "-----------------------------\n",
      "test_image_4.txt : 4 / 5\n",
      "['gas-station', 'atm', 'fire-station', 'ferris-wheel', 'bus-stop']\n",
      "{'gas': 0.18487394957983194, 'atm': 0.175, 'ferris': 0.12345679012345678, 'fire': 0.08333333333333333, 'water': 0.024}\n",
      "-----------------------------\n",
      "test_image_5.txt : 1 / 5\n",
      "['post-office', 'atm', 'telephone-booth', 'bike']\n",
      "{'bike': 0.2037037037037037, 'atm': 0.175, 'post': 0.07894736842105263, 'telephone': 0.06329113924050633}\n",
      "-----------------------------\n",
      "test_image_6.txt : 2 / 4\n",
      "['traffic-light', 'atm', 'university', 'fountain', 'car']\n",
      "{'atm': 0.175, 'university': 0.06521739130434782, 'traffic': 0.05172413793103448, 'car': 0.038461538461538464, 'lighthouse': 0.0}\n",
      "-----------------------------\n",
      "test_image_7.txt : 3 / 5\n",
      "['bridge', 'fountain', 'fire-station', 'university', 'telephone-booth']\n",
      "{'fire': 0.08333333333333333, 'telephone': 0.0759493670886076, 'bridge': 0.06086956521739131, 'university': 0.043478260869565216, 'barn': 0.01}\n",
      "-----------------------------\n",
      "test_image_8.txt : 2 / 5\n",
      "['car', 'police', 'cemetery', 'school']\n",
      "{'police': 0.2, 'cemetery': 0.18181818181818182, 'school': 0.1111111111111111, 'car': 0.038461538461538464}\n",
      "-----------------------------\n",
      "test_image_9.txt : 4 / 4\n",
      "Accuracy: 0.928\n"
     ]
    }
   ],
   "source": [
    "#flann with lowes\n",
    "sift = SIFT(train_folder=task_2_train, \n",
    "            test_folder=task_2_test, \n",
    "            test_annotation_folder=task_2_test_annotations, \n",
    "            matcher=cv2.FlannBasedMatcher_create(),\n",
    "            lowe_thresh=0.3, min_matches=3, n_feat=1000, n_octaves=5, c_thresh=0.1)\n",
    "\n",
    "sift.get_annotations()\n",
    "sift.get_sift_train_features()\n",
    "sift.eval(normalise=True ,lowes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sift_train_features(train_folder, nOctave, nFeat, contrast):\n",
    "    \n",
    "    train_points = {}\n",
    "    train_descriptors = {}\n",
    "    \n",
    "    sift = cv2.SIFT_create(nOctaveLayers=nOctave, nfeatures=nFeat, contrastThreshold=contrast)\n",
    "\n",
    "    for train_image in os.listdir(train_folder):\n",
    "        image = cv2.imread(os.path.join(train_folder, train_image), 0)\n",
    "        \n",
    "        pt, desc = sift.detectAndCompute(image, None)\n",
    "        # print(len(pt))\n",
    "        train_points[train_image] = pt\n",
    "        train_descriptors[train_image] = desc\n",
    "\n",
    "    return train_descriptors, train_points\n",
    "\n",
    "# get_sift_train_features(task_2_train, 3, 1000, 0.03)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_annotations(annotation_folder):\n",
    "    annotations = {}\n",
    "\n",
    "    for annotation_file in os.listdir(annotation_folder):\n",
    "        image_name = os.path.basename(annotation_file)\n",
    "        annotation_file = open(os.path.join(annotation_folder, annotation_file), \"r\")\n",
    "        annotation_lines = annotation_file.readlines()\n",
    "        image_annot = []\n",
    "        for line in annotation_lines:\n",
    "            image_annot.append(line.split(\",\")[0])\n",
    "        annotations[image_name] = image_annot\n",
    "    return annotations\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sift_matches(train_pt, train_desc, test_image_path, cv2Matcher, ratioThresh, nOctave, nFeat, contrastThresh, edgeThresh, sig):\n",
    "\n",
    "    test_image = cv2.imread(test_image_path, 0)\n",
    "\n",
    "    sift = cv2.SIFT_create(nOctaveLayers=nOctave, nfeatures=nFeat, contrastThreshold=contrastThresh, edgeThreshold=edgeThresh, sigma=sig)\n",
    "\n",
    "    test_points, test_descriptor = sift.detectAndCompute(test_image, None)\n",
    "    \n",
    "    matches = cv2Matcher.knnMatch(train_desc, test_descriptor, 2)\n",
    "    retained_matches = []\n",
    "    min_matches=3\n",
    "    for one, two in matches:\n",
    "        #change threshold; lower to reduce matches retained\n",
    "        if one.distance < ratioThresh*two.distance:\n",
    "            retained_matches.append([one])\n",
    "    \n",
    "    if len(retained_matches) > min_matches:\n",
    "        print(f\"Retained matches: {len(retained_matches)}\")\n",
    "        src_pts = np.float32([train_pt[m[0].queryIdx].pt for m in retained_matches]).reshape(-1, 1, 2)\n",
    "        dst_pts = np.float32([test_points[m[0].trainIdx].pt for m in retained_matches]).reshape(-1, 1, 2)\n",
    "        H, mask = cv2.findHomography(src_pts, dst_pts, cv2.RANSAC, 5.0)\n",
    "        \n",
    "        # print(\"Homography matrix:\\n\", H)\n",
    "        return H, mask\n",
    "    else:\n",
    "        #print(\"Not enough matches found\")\n",
    "\n",
    "        return None, None\n",
    "        print(\"Not enough matches found\")\n",
    " \n",
    "    return retained_matches\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_sift(train_folder, test_image_folder, test_annotation_folder):\n",
    "    total_total = 0\n",
    "    total_correct = 0\n",
    "    for test_image in os.listdir(test_image_folder):\n",
    "        \n",
    "        train_pts, train_desc = get_sift_train_features(train_folder, nOctave=5, nFeat=1000, contrast=0.1)\n",
    "        \n",
    "        scores = {}\n",
    "        \n",
    "        for image, desc in train_desc.items():\n",
    "            \n",
    "            \n",
    "            matcher = cv2.BFMatcher_create()#set crossCheck==true as alternative to ratio test\n",
    "            # matches = sift_matches(train_desc[image], train_pts[image], os.path.join(test_image_folder, test_image), matcher, ratioThresh=0.3, nOctave=3, nFeat=0, contrastThresh=0.04, edgeThresh=10, sig=1.6)\n",
    "            H,mask = sift_matches(train_desc[image], train_pts[image], os.path.join(test_image_folder, test_image), matcher, ratioThresh=0.75, nOctave=3, nFeat=10, contrastThresh=0.04, edgeThresh=10, sig=1.6)\n",
    "            if H is None:\n",
    "                scores[image.split(\"-\")[1].split(\".\")[0]] = 0\n",
    "            else:\n",
    "                canvas = cv2.imread(test_image_folder + test_image)\n",
    "                matches_mask = mask.ravel().tolist()\n",
    "                inliers = [bool(val) for val in matches_mask]\n",
    "\n",
    "                h, w = canvas.shape[:2]\n",
    "                pts = np.float32([[0, 0], [0, h - 1], [w - 1, h - 1], [w - 1, 0]]).reshape(-1, 1, 2)\n",
    "                print(f\"H : {H}\")\n",
    "                dst = cv2.perspectiveTransform(pts, H)\n",
    "                draw_params = dict(matchColor = (0,255,0),\n",
    "                                   singlePointColor = None,\n",
    "                                   matchesMask = matches_mask,\n",
    "                                   flags=2)\n",
    "                \n",
    "                print(f\"DST: {dst}\")\n",
    "                # Draw the matched features and bounding box\n",
    "                result = cv2.polylines(canvas, [np.int32(dst)], True, (0,0,255),2, cv2.LINE_AA)\n",
    "                # result = cv2.drawMatches(image, kp1, img2, kp2, good, inliers)\n",
    "                cv2.imshow(\"result\", result)\n",
    "                cv2.waitKey()\n",
    "                break\n",
    "                scores[image.split(\"-\")[1].split(\".\")[0]] = len(inliers)\n",
    "        \n",
    "            # scores[image.split(\"-\")[1].split(\".\")[0]] = len(matches)\n",
    "\n",
    "    #     scores = dict(sorted(scores.items(), key=lambda x: x[1], reverse=True))\n",
    "    #     annotations = get_annotations(test_annotation_folder)\n",
    "\n",
    "    #     correct, total = get_metrics(annotations, scores, test_image)\n",
    "    #     total_total+=total\n",
    "    #     total_correct+=correct\n",
    "    # print(\"Total performance:\", total_correct, \"/\", total_total)\n",
    "    # print(total_correct/total_total)\n",
    "                \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retained matches: 4\n",
      "(3, 3)\n",
      "H : [[ nan  nan  nan]\n",
      " [ inf -inf -inf]\n",
      " [ nan  nan  nan]]\n",
      "DST: [[[0. 0.]]\n",
      "\n",
      " [[0. 0.]]\n",
      "\n",
      " [[0. 0.]]\n",
      "\n",
      " [[0. 0.]]]\n",
      "Retained matches: 6\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\ds230\\Desktop\\Computer Vision Labs\\Git Repo\\ComputerVisionCoursework-1\\Task_3 .ipynb Cell 12\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/ds230/Desktop/Computer%20Vision%20Labs/Git%20Repo/ComputerVisionCoursework-1/Task_3%20.ipynb#X14sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m evaluate_sift(task_2_train, task_2_test, task_2_test_annotations)\n",
      "\u001b[1;32mc:\\Users\\ds230\\Desktop\\Computer Vision Labs\\Git Repo\\ComputerVisionCoursework-1\\Task_3 .ipynb Cell 12\u001b[0m in \u001b[0;36mevaluate_sift\u001b[1;34m(train_folder, test_image_folder, test_annotation_folder)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ds230/Desktop/Computer%20Vision%20Labs/Git%20Repo/ComputerVisionCoursework-1/Task_3%20.ipynb#X14sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m matcher \u001b[39m=\u001b[39m cv2\u001b[39m.\u001b[39mBFMatcher_create()\u001b[39m#set crossCheck==true as alternative to ratio test\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ds230/Desktop/Computer%20Vision%20Labs/Git%20Repo/ComputerVisionCoursework-1/Task_3%20.ipynb#X14sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m \u001b[39m# matches = sift_matches(train_desc[image], train_pts[image], os.path.join(test_image_folder, test_image), matcher, ratioThresh=0.3, nOctave=3, nFeat=0, contrastThresh=0.04, edgeThresh=10, sig=1.6)\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/ds230/Desktop/Computer%20Vision%20Labs/Git%20Repo/ComputerVisionCoursework-1/Task_3%20.ipynb#X14sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m H,mask \u001b[39m=\u001b[39m sift_matches(train_desc[image], train_pts[image], os\u001b[39m.\u001b[39;49mpath\u001b[39m.\u001b[39;49mjoin(test_image_folder, test_image), matcher, ratioThresh\u001b[39m=\u001b[39;49m\u001b[39m0.75\u001b[39;49m, nOctave\u001b[39m=\u001b[39;49m\u001b[39m3\u001b[39;49m, nFeat\u001b[39m=\u001b[39;49m\u001b[39m10\u001b[39;49m, contrastThresh\u001b[39m=\u001b[39;49m\u001b[39m0.04\u001b[39;49m, edgeThresh\u001b[39m=\u001b[39;49m\u001b[39m10\u001b[39;49m, sig\u001b[39m=\u001b[39;49m\u001b[39m1.6\u001b[39;49m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ds230/Desktop/Computer%20Vision%20Labs/Git%20Repo/ComputerVisionCoursework-1/Task_3%20.ipynb#X14sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m \u001b[39mif\u001b[39;00m H \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ds230/Desktop/Computer%20Vision%20Labs/Git%20Repo/ComputerVisionCoursework-1/Task_3%20.ipynb#X14sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m     scores[image\u001b[39m.\u001b[39msplit(\u001b[39m\"\u001b[39m\u001b[39m-\u001b[39m\u001b[39m\"\u001b[39m)[\u001b[39m1\u001b[39m]\u001b[39m.\u001b[39msplit(\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m)[\u001b[39m0\u001b[39m]] \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n",
      "\u001b[1;32mc:\\Users\\ds230\\Desktop\\Computer Vision Labs\\Git Repo\\ComputerVisionCoursework-1\\Task_3 .ipynb Cell 12\u001b[0m in \u001b[0;36msift_matches\u001b[1;34m(train_pt, train_desc, test_image_path, cv2Matcher, ratioThresh, nOctave, nFeat, contrastThresh, edgeThresh, sig)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ds230/Desktop/Computer%20Vision%20Labs/Git%20Repo/ComputerVisionCoursework-1/Task_3%20.ipynb#X14sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m dst_pts \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mfloat32([test_points[m[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mtrainIdx]\u001b[39m.\u001b[39mpt \u001b[39mfor\u001b[39;00m m \u001b[39min\u001b[39;00m retained_matches])\u001b[39m.\u001b[39mreshape(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, \u001b[39m1\u001b[39m, \u001b[39m2\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ds230/Desktop/Computer%20Vision%20Labs/Git%20Repo/ComputerVisionCoursework-1/Task_3%20.ipynb#X14sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m H, mask \u001b[39m=\u001b[39m cv2\u001b[39m.\u001b[39mfindHomography(src_pts, dst_pts, cv2\u001b[39m.\u001b[39mRANSAC, \u001b[39m5.0\u001b[39m)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/ds230/Desktop/Computer%20Vision%20Labs/Git%20Repo/ComputerVisionCoursework-1/Task_3%20.ipynb#X14sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m \u001b[39mprint\u001b[39m(H\u001b[39m.\u001b[39;49mshape)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ds230/Desktop/Computer%20Vision%20Labs/Git%20Repo/ComputerVisionCoursework-1/Task_3%20.ipynb#X14sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m \u001b[39m# print(\"Homography matrix:\\n\", H)\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ds230/Desktop/Computer%20Vision%20Labs/Git%20Repo/ComputerVisionCoursework-1/Task_3%20.ipynb#X14sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m \u001b[39mreturn\u001b[39;00m H, mask\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "evaluate_sift(task_2_train, task_2_test, task_2_test_annotations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[WinError 3] The system cannot find the path specified: 'Task3AdditionalTestDataset/images/'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\ds230\\Desktop\\Computer Vision Labs\\Git Repo\\ComputerVisionCoursework-1\\Task_3 .ipynb Cell 13\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/ds230/Desktop/Computer%20Vision%20Labs/Git%20Repo/ComputerVisionCoursework-1/Task_3%20.ipynb#X15sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m evaluate_sift(task_2_train, task_3_test, task_3_test_annotations)\n",
      "\u001b[1;32mc:\\Users\\ds230\\Desktop\\Computer Vision Labs\\Git Repo\\ComputerVisionCoursework-1\\Task_3 .ipynb Cell 13\u001b[0m in \u001b[0;36mevaluate_sift\u001b[1;34m(train_folder, test_image_folder, test_annotation_folder)\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/ds230/Desktop/Computer%20Vision%20Labs/Git%20Repo/ComputerVisionCoursework-1/Task_3%20.ipynb#X15sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m total_total \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/ds230/Desktop/Computer%20Vision%20Labs/Git%20Repo/ComputerVisionCoursework-1/Task_3%20.ipynb#X15sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m total_correct \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/ds230/Desktop/Computer%20Vision%20Labs/Git%20Repo/ComputerVisionCoursework-1/Task_3%20.ipynb#X15sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mfor\u001b[39;00m test_image \u001b[39min\u001b[39;00m os\u001b[39m.\u001b[39;49mlistdir(test_image_folder):\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/ds230/Desktop/Computer%20Vision%20Labs/Git%20Repo/ComputerVisionCoursework-1/Task_3%20.ipynb#X15sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     train_pts, train_desc \u001b[39m=\u001b[39m get_sift_train_features(train_folder, nOctave\u001b[39m=\u001b[39m\u001b[39m5\u001b[39m, nFeat\u001b[39m=\u001b[39m\u001b[39m1000\u001b[39m, contrast\u001b[39m=\u001b[39m\u001b[39m0.1\u001b[39m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/ds230/Desktop/Computer%20Vision%20Labs/Git%20Repo/ComputerVisionCoursework-1/Task_3%20.ipynb#X15sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m     scores \u001b[39m=\u001b[39m {}\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 3] The system cannot find the path specified: 'Task3AdditionalTestDataset/images/'"
     ]
    }
   ],
   "source": [
    "evaluate_sift(task_2_train, task_3_test, task_3_test_annotations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##old github post\n",
    "import matplotlib, cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import math\n",
    "import os\n",
    "import itertools\n",
    "import random\n",
    "%matplotlib inline\n",
    "task_2_train = \"Task2Dataset/Training/png/\"\n",
    "task_2_test = \"Task2Dataset/TestWithoutRotations/images/\"\n",
    "task_2_test_annotations = \"Task2Dataset/TestWithoutRotations/annotations/\"\n",
    "\n",
    "task_3_test_annotations = \"Task3AdditionalTestDataset/annotations/\"\n",
    "task_3_test = \"Task3AdditionalTestDataset/images/\"\n",
    "\n",
    "def get_train_features(train_folder):\n",
    "    \n",
    "    train_points = {}\n",
    "    train_descriptors = {}\n",
    "    \n",
    "    sift = cv2.SIFT_create(nOctaveLayers=5, nfeatures=1000, contrastThreshold=0.1)\n",
    "\n",
    "    for train_image in os.listdir(train_folder):\n",
    "        image = cv2.imread(os.path.join(train_folder, train_image), 0)\n",
    "        \n",
    "        pt, desc = sift.detectAndCompute(image, None)\n",
    "        \n",
    "        train_points[train_image] = pt\n",
    "        train_descriptors[train_image] = desc\n",
    "\n",
    "    return train_descriptors, train_points\n",
    "\n",
    "def get_annotations(annotation_folder):\n",
    "    annotations = {}\n",
    "\n",
    "    for annotation_file in os.listdir(annotation_folder):\n",
    "        image_name = os.path.basename(annotation_file)\n",
    "        annotation_file = open(os.path.join(annotation_folder, annotation_file), \"r\")\n",
    "        annotation_lines = annotation_file.readlines()\n",
    "        image_annot = []\n",
    "        for line in annotation_lines:\n",
    "            image_annot.append(line.split(\",\")[0])\n",
    "        annotations[image_name] = image_annot\n",
    "    return annotations\n",
    "\n",
    "def sift_matches(train_pt, train_desc, test_image_path):\n",
    "\n",
    "    test_image = cv2.imread(test_image_path, 0)\n",
    "\n",
    "    sift = cv2.SIFT_create(nOctaveLayers=5, nfeatures=1000, contrastThreshold=0.1)\n",
    "\n",
    "    test_points, test_descriptor = sift.detectAndCompute(test_image, None)\n",
    "\n",
    "    matcher = cv2.BFMatcher_create()\n",
    "\n",
    "    matches = matcher.knnMatch(train_desc, test_descriptor, 2)\n",
    "    retained_matches = []\n",
    "    for one, two in matches:\n",
    "        #change threshold; lower to reduce matches retained\n",
    "        if one.distance < 0.3*two.distance:\n",
    "            retained_matches.append([one])\n",
    "\n",
    "    return retained_matches\n",
    " \n",
    "def evaluate(train_folder, test_image_folder, test_annotation_folder):\n",
    "    total_total = 0\n",
    "    total_correct = 0\n",
    "    for test_image in os.listdir(test_image_folder):\n",
    "        \n",
    "        train_pts, train_desc = get_train_features(train_folder)\n",
    "        \n",
    "        scores = {}\n",
    "        \n",
    "        for image, desc in train_desc.items():\n",
    "\n",
    "            matches = sift_matches(train_desc[image], train_pts[image], os.path.join(test_image_folder, test_image))\n",
    "            scores[image.split(\"-\")[1].split(\".\")[0]] = len(matches)\n",
    "\n",
    "        scores = dict(sorted(scores.items(), key=lambda x: x[1], reverse=True))\n",
    "        annotations = get_annotations(test_annotation_folder)\n",
    "\n",
    "        scores = dict(itertools.islice(scores.items(), len(annotations)))\n",
    "        print(scores)\n",
    "        # note - need to refactor task 3 test annotations from .csv to .txt before use\n",
    "        test_annotations = annotations[test_image.split(\".\")[0]+\".txt\"]\n",
    "        true_positives, false_positives, true_negatives, false_negatives = 0,0,0,0 \n",
    "        for annotation in test_annotations:\n",
    "            if annotation in scores:\n",
    "                true_positives += 1\n",
    "            else:\n",
    "                false_positives += 1\n",
    "                false_negatives += 1\n",
    "        total = false_positives + true_positives\n",
    "        total_total+=total\n",
    "        total_correct+=true_positives\n",
    "        print(test_image, \":\", true_positives, \"/\", total)\n",
    "    print(\"Total performance:\", total_correct, \"/\", total_total)\n",
    "                \n",
    "\n",
    "\n",
    "evaluate(task_2_train, task_2_test, task_2_test_annotations)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cm30080",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
